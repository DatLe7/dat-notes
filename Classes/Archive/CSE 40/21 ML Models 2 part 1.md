### Date: 03-3-2025
### Instructor: Alexander Rudnick


## Notes:

### Linear vs. Non-Linear Models
![[Pasted image 20250310161642.png]]
### Two Approaches to Learning
![[Pasted image 20250310161712.png]]

### K-Nearest Neighbor
**Simple idea:**
- Store all training examples.
- Classify new examples based on finding closest (presumably most similar) training examples.

**Key design decisions:**
- How many neighbors to look at? (k parameter)
- What distance metric to use?

**Quote:**
This ‚Äúrule of nearest neighbor‚Äù has considerable elementary intuitive appeal and probably corresponds to practice in many situations. For example, it is possible that much medical diagnosis is influenced by the doctor‚Äôs recollection of the subsequent history of an earlier patient whose symptoms resemble in some way those of the current patient.
				(Fix and Hodges, 1952) 

#### Components of a K-NN Classifier
**Distance metric**
- How do we measure similarity between instances?
- Determines the layout of the example space.

**The k hyperparameter**
- How large a neighborhood should we consider?
- Determines the complexity of the hypothesis space.

#### Different Distance Metrics
![[Pasted image 20250311132956.png]]
#### Different Decision Boundaries for Different Hyperparameters of KNN
![[Pasted image 20250311133048.png]]
![[Pasted image 20250311133055.png]]

#### K Hyperparameter
- Tunes the complexity of the hypothesis space:
	- If k = 1, every training example has its own neighborhood.
	- If k = N, the entire feature space is one neighborhood!
- Higher k yields smoother decision boundaries.
- Challenge: How to set k in practice?

#### Assumptions of k-NN?
- Nearby instances should have the same label.
- All features are equally important.
- Complexity is tuned by the k parameter.

#### Variation of KNN
**Weighted voting:**
- Default: all neighbors have equal weight.
- Extension: weight neighbors by (inverse) distance.

**Epsilon Ball Nearest Neighbors:**
- Same general principle as k-NN, but change the method for selecting which training examples vote.
- Instead of using k nearest neighbors, use all examples x‚Äô such that for some small value ùúñ
![[Pasted image 20250311133359.png]]

### Decision Trees
**Simple idea:**
- Build a tree that splits according to different feature values
- Make classification for new example by following decision tree splits to leaf
- Classify new example based on labels at the leaf

**Key design decisions:**
- What is the best feature to split on?
- When to stop growing the tree?
![[Pasted image 20250311133516.png]]

#### Learning Decision Trees
- One of the simplest, yet most successful learning algorithms
- Easy to implement
- Inputs ‚Äì discrete or continuous
- Outputs ‚Äì discrete (classification) or continuous (regression)
- Decision tree nodes represent sequence of tests
- Decision tree representation is very natural for humans ‚Äì e.g., ‚ÄúHow To‚Äù manuals 

![[Pasted image 20250311133634.png]]

#### Expressiveness
- Decision trees can express any function of the input attributes.
- E.g., for Boolean functions, truth table row ‚Üí path to leaf:
![[Pasted image 20250311133716.png]]
- Trivially, there is a consistent decision tree for any training set with one path to the leaf for each example (unless f nondeterministic in x), but it probably won't generalize well to new examples.
- Prefer to find more compact decision trees to permit generalization and prevent overfitting.

#### Hypothesis Space of Decision Trees
**How many distinct functions with n Boolean attributes?**
= number of Boolean functions
= number of distinct truth tables with $2^n$ rows = $2^{2n}$
- For 10 attributes, $2^{1024}$ or about $10^{308}$ different functions; for 20 attributes, $10^{300,000}$!!
- How many trees? Even more than the number of functions, since more than one tree can represent the same function!!

#### What Makes a Good Tree
**Small**
- Simpler is better
- Ockham‚Äôs razor
- Avoids overfitting
**A decision tree may be human readable, but not use human logic.**
- The decision tree you would write for a problem may differ from what is generated by a decision tree learning algorithm.

### How do we find Good Trees

#### Greedy Algorithms
- As with many NP-complete problems, we can get pretty good solutions much more efficiently.
- Most decision tree learning uses **greedy algorithms**.
	- Myopically find the best split at each level of recursion.
	- Adjustments are usually needed to repair flaws with greedy selection problems‚Ä¶
	- ‚Ä¶ but that‚Äôs still better than using an impractically inefficient optimization method!
- Top-down decision tree learning
	- Recursive algorithm

